{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m1.26541\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1000 | loss: 1.26541 - acc: 0.9325 -- iter: 24/27\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m1.13950\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1000 | loss: 1.13950 - acc: 0.9392 -- iter: 27/27\n",
      "--\n",
      " Model is Trained!\n",
      "INFO:tensorflow:/Users/danieldsouza/Desktop/Academic/Code/Python/ChatBot/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n",
      " Model Saved + Pickles Pickled!\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#################################################\n",
    "## Author : Daniel D'souza\n",
    "## Email : ddsouza@umich.edu\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "###### Libraries #######\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "## Importing the Lancaster Stemmer fron NLTK to stem our words\n",
    "from nltk.stem.lancaster import  LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "## Import Our Intent File( Our Chatbot Brain)\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "#################################################\n",
    "#### PART 1 : Unpacking Intent and Data Prep ####\n",
    "#################################################\n",
    "\n",
    "## Code to unpack the Intents file and use its contents\n",
    "# All possible words in your questions\n",
    "words = []\n",
    "# A tuple of 'words associated' and 'categories' for all questions\n",
    "documents = []\n",
    "# All possible categories that your chatbot can carry a conversation in\n",
    "classes = []\n",
    "# Ignoring these words\n",
    "ignore_words = ['?']\n",
    "\n",
    "# Loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in your questions list(patterns)\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Add it to all possible words list\n",
    "        words.extend(w)\n",
    "        # Add the Tuple of (w,tag) to documents list\n",
    "        documents.append((w,intent['tag']))\n",
    "        # Also add it to your classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "\n",
    "## Code to prepare data\n",
    "# Lowercasing, Stemming and Deduping your word list\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# Deduping your classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# Print an Update of the Information collected\n",
    "print(\"*\"*50)\n",
    "print(len(documents), \" Documents \")\n",
    "print(len(classes), \" Classes \", classes)\n",
    "print(len(words),\" Words \", words)\n",
    "print(\"*\"*50)\n",
    "\n",
    "\n",
    "## Data Munging\n",
    "# Creating Training and Output Lists\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "# Creating an empty array for our output\n",
    "output_empty = [0]*len(classes)\n",
    "\n",
    "# Converting our queriers into BOW format\n",
    "for doc in documents:\n",
    "    # doc is now a tuple of tokenized 'words associated' and 'category'\n",
    "    # Create an empy bag\n",
    "    bag = []\n",
    "    # Get your tokenized words out\n",
    "    pattern_words = doc[0]\n",
    "    # Stem them\n",
    "    pattern_words = [stemmer.stem(word) for word in pattern_words ]\n",
    "    # Create a BOW representation\n",
    "    for w in words:\n",
    "        # 1 for every present word, 0 for every absent one\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # Create your respective output vector\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    # Append this newly created training input and output sample into your training list\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Shuffle your Trainining Features, coz why not?\n",
    "random.shuffle(training)\n",
    "\n",
    "# Package it into an Numpy Representation for easy compatibility\n",
    "training = np.array(training)\n",
    "\n",
    "# Split your training list into input x and output y\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "\n",
    "#################################################\n",
    "#### PART 2 : Training the Chatbot Model ####\n",
    "#################################################\n",
    "\n",
    "# Reset the Underlying Graph Data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Network Architechture\n",
    "# Input Layer : BOW_sizex1\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "# Fully_Connected Layer(FC1) : 8x1\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "# Fully_Connected Layer(FC2) : 8x1\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "# Fully_Connected Output(FC1) : Classes_sizex1 : (Softmax Activated)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "# Regression\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "## Define Model and Tensorboard Setup for Visualization\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "## Start Training ( Use the Gradient Descent Algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "\n",
    "print(\" Model is Trained!\")\n",
    "## Save our model and pickle the data_structures for the Chatbot to use\n",
    "model.save('model.tflearn')\n",
    "\n",
    "pickle.dump({'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open(\"train_data\", \"wb\"))\n",
    "\n",
    "print(\" Model Saved + Pickles Pickled!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "#################################################\n",
    "## Author : Daniel D'souza\n",
    "## Email : ddsouza@umich.edu\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "###### Libraries #######\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "## Restore the Trained model and the Data Structures\n",
    "\n",
    "data = pickle.load( open(\"train_data\", \"rb\"))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "## Import our ChatBot Intents File\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "## Load the Model\n",
    "# Reset the Underlying Graph Data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Network Architechture\n",
    "# Input Layer : BOW_sizex1\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "# Fully_Connected Layer(FC1) : 8x1\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "# Fully_Connected Layer(FC2) : 8x1\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "# Fully_Connected Output(FC1) : Classes_sizex1 : (Softmax Activated)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "# Regression\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "## Define Model and Tensorboard Setup for Visualization\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "model.load('./model.tflearn')\n",
    "\n",
    "\n",
    "#################################################\n",
    "#### The Response and Classificaton Code  ####\n",
    "#################################################\n",
    "#\n",
    "# We need 4 functions here to complete our ChatBot Functionality:\n",
    "# 1. clean_up_sent() : To clean up your sentences by lowercasing and stemming\n",
    "# 2. create_bow() : To return a BOW representation of the query to input into the trained model for classification\n",
    "# 3. classify() : To perform the actual classification on the sentence\n",
    "# 4. response(): Tie everything up tidily and print out a response\n",
    "#\n",
    "#################################################\n",
    "\n",
    "# Error Threshold\n",
    "ERROR_THRESHOLD = 0.25\n",
    "\n",
    "#Context\n",
    "context = {}\n",
    "\n",
    "def clean_up_sent(sentence):\n",
    "    # Tokenize your sentence\n",
    "    sentence_words = nltk.word_tokenize(sentence, language='english')\n",
    "    # Stem each word\n",
    "    sentence_words = [stemmer.stem(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def create_bow(sentence, words,show_details=False):\n",
    "    # Tokenize and Stem the Query\n",
    "    sentence_words = clean_up_sent(sentence)\n",
    "\n",
    "    # BOW Representation\n",
    "    bag = [0]*len(words)\n",
    "\n",
    "    for query_word in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == query_word:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: {0}\".format(w))\n",
    "\n",
    "    return np.array(bag)\n",
    "\n",
    "def classify(sentence):\n",
    "    # Generate the Prediction of the Query\n",
    "    results = model.predict([create_bow(sentence, words)])[0]\n",
    "    # Filter out predictions beyond your previously defined Error Threshold\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # Return tuple of Intent and Probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "\n",
    "    #Find your matching intent tag\n",
    "    if results:\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # now return a random answer for that category\n",
    "                    if 'context_set' in i:\n",
    "                        context[userID] = i['context_set']\n",
    "                        if show_details: \n",
    "                            print('context:', i['context_set'])\n",
    "\n",
    "                    if('context_filter' not in i or (userID in context and 'context_filter' in i \\\n",
    "                                                             and context[userID] == i['context_filter'])):\n",
    "                        return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're open every day from 9am-9pm\n"
     ]
    }
   ],
   "source": [
    "response('what time are you guys open today ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
